

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/inf.png">
  <link rel="icon" href="/img/inf.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="thj2333">
  <meta name="keywords" content="">
  
    <meta name="description" content="NeRF survey2020年-2022年CVPR中，NeRF（神经辐射场）这个话题爆发出强大的创造力，从神经渲染的定义（“深度图像或视频生成方法可以显式或隐式地控制场景属性，例如照明、相机参数、姿势、几何形状、外观和语义结构。”）出发，这是一种新颖的数据驱动解决方案，用于解决计算机图形学中长期存在的虚拟世界真实渲染问题。MIT CSAIL的博士生Yen-Chen Lin在Github上策划的a">
<meta property="og:type" content="article">
<meta property="og:title" content="NeRF survey(2020)">
<meta property="og:url" content="https://thj2333.github.io/2022/08/20/NeRF-2020/index.html">
<meta property="og:site_name" content="thjEarth-616">
<meta property="og:description" content="NeRF survey2020年-2022年CVPR中，NeRF（神经辐射场）这个话题爆发出强大的创造力，从神经渲染的定义（“深度图像或视频生成方法可以显式或隐式地控制场景属性，例如照明、相机参数、姿势、几何形状、外观和语义结构。”）出发，这是一种新颖的数据驱动解决方案，用于解决计算机图形学中长期存在的虚拟世界真实渲染问题。MIT CSAIL的博士生Yen-Chen Lin在Github上策划的a">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://thj2333.github.io/img/19.jpg">
<meta property="article:published_time" content="2022-08-20T08:56:18.250Z">
<meta property="article:modified_time" content="2022-08-20T09:50:07.806Z">
<meta property="article:author" content="thj2333">
<meta property="article:tag" content="NeRF">
<meta property="article:tag" content="survey">
<meta property="article:tag" content="CV">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://thj2333.github.io/img/19.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>NeRF survey(2020) - thjEarth-616</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"thj2333.github.io","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>thjEarth-616</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/19.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="NeRF survey(2020)"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-08-20 16:56" pubdate>
          2022年8月20日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          48 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">NeRF survey(2020)</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：几秒前
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <h1 id="NeRF-survey"><a href="#NeRF-survey" class="headerlink" title="NeRF survey"></a>NeRF survey</h1><p>2020年-2022年CVPR中，NeRF（神经辐射场）这个话题爆发出强大的创造力，从神经渲染的定义（<em>“深度图像或视频生成方法可以显式或隐式地控制场景属性，例如照明、相机参数、姿势、几何形状、外观和语义结构。”</em>）出发，这是一种新颖的数据驱动解决方案，用于解决计算机图形学中长期存在的<strong>虚拟世界真实渲染问题</strong>。MIT CSAIL的博士生Yen-Chen Lin在Github上策划的<a target="_blank" rel="noopener" href="https://github.com/yenchenlin/awesome-NeRF">awesome-NeRF</a>项目整理了相关论文及工作，同时感谢Professor Frank Dellaert的blog整理了近三年CVPR,ECCV,ICCV的相关工作。下面是一些前置概念及工作：</p>
<h2 id="The-Prelude-Neural-Implicit-Surfaces"><a href="#The-Prelude-Neural-Implicit-Surfaces" class="headerlink" title="The Prelude: Neural Implicit Surfaces"></a>The Prelude: Neural Implicit Surfaces</h2><p><strong>Neural Implicit Surfaces</strong>：神经体绘制的直接前身是使用神经网络定义<strong>隐式</strong>表面表示的方法。许多 3D 感知图像生成方法使用体素、网格、点云或其他表示，通常基于卷积架构。但在 CVPR 2019 上，不少于三篇论文介绍了使用神经网络作为<em>标量函数逼近</em>器来定义占用和/或有符号距离函数。</p>
<h3 id="Occupancy-networks"><a href="#Occupancy-networks" class="headerlink" title="Occupancy networks"></a><strong>Occupancy networks</strong></h3><p>占用网络是 CVPR 2019 中引入隐式、基于坐标的<strong>占用</strong>学习的两种方法之一。由 5 个 ResNet 块组成的网络采用特征向量和 3D 点并预测二进制占用。他们还展示了来自 KITTI 的真实图像的单视图重建结果。<em><strong>Occupancy Networks: Learning 3D Reconstruction in Function Space</strong>，CVPR 2019</em></p>
<h3 id="IM-NET"><a href="#IM-NET" class="headerlink" title="IM-NET"></a><strong>IM-NET</strong></h3><p>它使用 6 层 MLP 解码器，在给定特征向量和 3D 坐标的情况下预测二进制占用。作者表明，这种“隐式解码器”可用于自动编码、形状生成（GAN 风格）和单视图重建。【<a target="_blank" rel="noopener" href="https://github.com/czq142857/implicit-decoder">Github</a>】</p>
<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2022/08/20/NeRF-2020/1.png" srcset="/img/loading.gif" lazyload width="80%" alt>    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">      使用 GAN 生成的 3D 形状，使用 IM-NET 作为解码器     </div> </center>

<h3 id="DeepSDF"><a href="#DeepSDF" class="headerlink" title="DeepSDF"></a><strong>DeepSDF</strong></h3><p>在 CVPR 2019 上，<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/DeepSDF">DeepSDF</a>从 3D 坐标和可选的潜在代码中直接回归有<strong>符号距离函数</strong>或 SDF，而不是二进制占用。它使用 8 层 MLP 与第 4 层的 skip-connections 输出有符号距离。</p>
<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2022/08/20/NeRF-2020/2.png" srcset="/img/loading.gif" lazyload width="50%" alt>    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">      通过学习有符号距离函数 (SDF) 渲染的斯坦福兔      </div> </center>

<h3 id="PIFu"><a href="#PIFu" class="headerlink" title="PIFu"></a><strong>PIFu</strong></h3><p>在此基础上，ICCV 2019 <a target="_blank" rel="noopener" href="https://shunsukesaito.github.io/PIFu/">PIFu</a>论文表明，可以通过将 3D 点重新投影到像素对齐的特征表示中来学习高度详细的隐式模型。这个想法稍后将在 PixelNeRF 中重新出现，并产生巨大的影响。<strong>PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization</strong>, <em>ICCV 2019</em></p>
<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2022/08/20/NeRF-2020/3.png" srcset="/img/loading.gif" lazyload width="80%" alt>    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">      PIFu 从像素对齐的特征中回归颜色和 SDF，从而实现单视图重建      </div> </center>

<h3 id="Building-on-Implicit-Functions"><a href="#Building-on-Implicit-Functions" class="headerlink" title="Building on Implicit Functions"></a>Building on Implicit Functions</h3><p>其他几种建立在隐函数思想之上的方法，并推广到从 2D 图像进行训练。值得注意的是结构化隐式函数（Structured Implicit Functions）、CvxNet、BSP-Net、深度局部形状（Deep Local Shapes）、场景表示网络（Scene Representation Networks）、可微体积渲染（Differentiable Volumetric Rendering）、隐式可微渲染器（Implicit Differentiable Renderer）和 NASA。</p>
<p>同样发表在 ICCV 2019 上的<a target="_blank" rel="noopener" href="https://github.com/google/ldif">Structured Implicit Functions</a>表明您可以组合这些隐式表示，例如，只需将它们相加即可。组合有符号距离函数的另一种方法是采用逐点最大值（in 3D），如<a target="_blank" rel="noopener" href="https://cvxnet.github.io/">CvxNet</a>中所做的那样，该论文具有许多其他优雅的技术来从深度或 RGB 图像重建对象。<a target="_blank" rel="noopener" href="https://bsp-net.github.io/">BSP-Net</a>在许多方面与 CvxNet 相似，但在其核心使用二进制空间分区，产生一种本机输出多边形网格的方法，而不是通过昂贵的网格划分方法。最后，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.10983">Deep Local Shapes</a>将 DeepSDF 潜在代码存储在体素网格中，以表示更大的扩展场景。</p>
<p><a target="_blank" rel="noopener" href="https://vsitzmann.github.io/srns/">场景表示网络或 SRN</a>在架构方面与 DeepSDF 非常相似，但添加了可微的光线行进算法以找到学习的隐式曲面的最近交点，并添加 MLP 来回归颜色，使其能够从多个位姿的图像中学习。</p>
<p>与 SRN 工作类似，CVPR 2020<a target="_blank" rel="noopener" href="https://github.com/autonomousvision/differentiable_volumetric_rendering">可微分体积渲染</a>论文表明，隐式场景表示可以与可微分渲染器相结合，使其可从图像中训练。他们使用术语<em>体积渲染器</em>，但真正的主要贡献是一个巧妙的技巧，可以使隐式表面的深度计算可微：不使用体积上的积分。</p>
<p>Weizmann 在 NeurIPS 2020 上展示的<a target="_blank" rel="noopener" href="https://lioryariv.github.io/idr/">Implicit Differentiable Renderer</a>工作类似，但它具有更复杂的表面光场表示，并且作者还表明他们可以在训练期间改进相机姿势。</p>
<p>最后，神经关节形状近似<a target="_blank" rel="noopener" href="https://virtualhumans.mpi-inf.mpg.de/nasa/">Neural Articulated Shape Approximation</a>或 NASA 组合隐式函数来表示关节物体，例如人体。</p>
<h2 id="Neural-Volume-Rendering"><a href="#Neural-Volume-Rendering" class="headerlink" title="Neural Volume Rendering"></a>Neural Volume Rendering</h2><p><strong>Neural volume rendering</strong>（神经体渲染）：通过将光线追踪到场景中并在光线长度上进行某种积分来生成图像或视频的方法。通常，像MLP这样的神经网络会将光线上的 3D 坐标通过函数编码为密度（不透明度）和颜色等量，这些量被集成以产生图像。</p>
<p>2020年，有两篇论文将<strong>体渲染</strong>引入了视图合成领域，其中 NeRF 是最简单的，也是最有影响力的。</p>
<p><em>关于命名</em>：下面的两篇论文和所有 Nerf 风格的论文都建立在上面对隐式表面进行编码的工作之上，因此<em>隐式神经方法</em>（<em>implicit neural methods</em>）这个术语被大量使用。然而，我个人更多地将这个术语与曲线和曲面的水平集表示联系起来。它们与 occupancy/SDF-style 网络的共同点是 MLP 用作从 3D 坐标到标量或多变量场的函数，因此这些方法有时也称为<strong>基于坐标的</strong>（coordinate-based）场景表示网络。在这个更大的范围中，我们关注的是下面的<em>volume rendering</em>版本。</p>
<h3 id="Neural-Volumes"><a href="#Neural-Volumes" class="headerlink" title="Neural Volumes"></a>Neural Volumes</h3><p>AFAIK，用于视图合成的真实<strong>体积渲染</strong>，是在 Facebook Reality Labs 的<a target="_blank" rel="noopener" href="https://research.fb.com/publications/neural-volumes-learning-dynamic-renderable-volumes-from-images/">Neural Volumes</a>论文中引入的，它回归了密度和颜色的 3D volume，尽管仍然是基于体素的表示。</p>
<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2022/08/20/NeRF-2020/4.png" srcset="/img/loading.gif" lazyload width="90%" alt>    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">      在“Neural Volumes”方法中，潜码被解码为 3D volume，然后通过体渲染获得新图像     </div> </center>

<p><strong>Neural Volumes: Learning Dynamic Renderable Volumes from Images</strong>, <em>SIGGRAPH 2019</em></p>
<p>本文中最关键的引述之一是神经体绘制方法：</p>
<blockquote>
<p>[我们]建议在 3D 空间中的每个位置使用由不透明度和颜色组成的体积表示，其中通过积分投影实现渲染。在优化过程中，这种半透明的几何表示沿着积分线分散梯度信息，<strong>有效地扩大了收敛范围，从而能够发现好的解决方案</strong>。</p>
</blockquote>
<p>我认为这引起了很多人的共鸣，并部分解释了神经体绘制的成功。有关方法的详细介绍请参见论文，下面我们来深入了解一下 NeRF 。</p>
<h3 id="NeRF"><a href="#NeRF" class="headerlink" title="NeRF"></a>NeRF</h3><p>引起大家讨论的论文是<a target="_blank" rel="noopener" href="https://www.matthewtancik.com/nerf">Neural Radiance Fields 或 NeRF 论文</a>，三位第一作者来自伯克利。本质上，它们采用 DeepSDF 架构，但回归的不是有符号距离函数，而是密度和颜色。然后，他们使用（易于微分的）数值积分方法来近似真实的体积渲染步骤。</p>
<p>下图说明了整体设置和有关渲染过程的一些细节：</p>
<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2022/08/20/NeRF-2020/5.png" srcset="/img/loading.gif" lazyload width="90%" alt>    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">      NeRF 将体积场景表示存储为 MLP 的权重，在许多具有已知位姿的图像上进行训练      </div> </center>

<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2022/08/20/NeRF-2020/6.png" srcset="/img/loading.gif" lazyload width="90%" alt>    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">      通过沿每条视线定期集成密度和颜色来呈现新视图      </div> </center>

<p><strong>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</strong>, <em>ECCV 2020</em></p>
<p>NeRF 能够进行非常详细的渲染的原因之一是因为它使用周期性激活函数（即<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~bmild/fourfeat/">傅立叶特征</a>）对射线上的 3D 点和关联的视图方向进行编码。这一创新后来被推广到具有周期性激活的多层网络，即<a target="_blank" rel="noopener" href="https://vsitzmann.github.io/siren/">SIREN</a>（正弦表示网络）。两者均随后在 NeurIPS 2020 上发布。</p>
<p>虽然 NeRF 论文表面上发表在 ECCV 2020 上，但在 8 月底，它首次出现在 3 月中旬的 Arxiv上，引发了<a target="_blank" rel="noopener" href="https://twitter.com/ak92501/status/1240802419476508673?s=20">twitter</a>上的兴趣爆炸式增长。我对<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=JuH79E8rdKc">视频</a>中合成视图的质量感到惊讶，对可视化深度图中难以置信的细节更是如此，例如下面的一些视频展示：</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;"><iframe src="https://www.youtube.com/watch?v=JuH79E8rdKc" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; 
height: 100%; left: 0; top: 0;"> </iframe></div>

<p>更多细节请查看项目网站：<a target="_blank" rel="noopener" href="https://www.matthewtancik.com/nerf">https://www.matthewtancik.com/nerf</a></p>
<p>可以说，NeRF 论文的影响在于其简单性：只是一个 MLP 接受 5D 坐标并输出密度和颜色。尽管有一些花里胡哨的东西，特别是位置编码和分层采样方案，但许多研究人员还是对这样一个简单的架构可以产生如此令人印象深刻的结果感到震惊。话虽如此，vanilla NeRF 留下了许多改进的机会：</p>
<ul>
<li>训练和渲染都很慢</li>
<li>只能表示静态场景</li>
<li>It “bakes in” lighting</li>
<li>经过训练的 NeRF 表示不会泛化到其他场景/对象</li>
</ul>
<p>在这个由 Arxiv 推动的计算机视觉世界中，这些机会几乎立即得到了利用，在2020年，Arxiv 上出现了近 25 篇论文 ，下面列出并讨论了每个类别中的代表性论文。</p>
<h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><p>一些项目/论文旨在改善原始 NeRF 论文相当慢的训练和渲染时间。</p>
<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2022/08/20/NeRF-2020/7.png" srcset="/img/loading.gif" lazyload width="100%" alt>    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">      NSVF 论文使用八叉树作为数据结构，声称 Neural Volumes 和 NeRF 都是特例。如在 NeRF 中，每个体素顶点上的特征嵌入被插值并馈送到（较小的）NLP 输出密度和颜色      </div> </center>

<p><strong>Neural Sparse Voxel Fields</strong>, <em>NeurIPS 2020.</em></p>
<p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/NSVF">神经稀疏体素场</a>（见上图）将场景组织成稀疏体素八叉树，以将渲染速度提高 10 倍。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/Kai-46/nerfplusplus">NERF++</a>建议使用单独的 NeRF 对背景进行建模，以处理无界场景。</p>
<p><a target="_blank" rel="noopener" href="https://ubc-vision.github.io/derf/">DeRF</a>将场景分解为 “软 Voronoi 图” ，以利用加速器内存架构。</p>
<p><a target="_blank" rel="noopener" href="https://www.computationalimaging.org/publications/automatic-integration/">AutoInt</a>通过直接学习体积积分大大加快了渲染速度。<em>这是一篇有趣且更通用的论文</em></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.02189">Learned Initializations</a>使用元学习来找到一个好的权重初始化，以加快训练速度。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/google-research/google-research/tree/master/jaxnerf">JaxNeRF</a>使用 JAX (<a target="_blank" rel="noopener" href="https://github.com/google/jax">https://github.com/google/jax</a>) 将训练速度从几天缩短到几小时。</p>
<h2 id="Dynamic"><a href="#Dynamic" class="headerlink" title="Dynamic"></a>Dynamic</h2><p><a target="_blank" rel="noopener" href="https://nerfies.github.io/">Nerfies</a>及其底层的 D-NeRF 模型可变形视频，使用第二个 MLP 为视频的每一帧应用变形。</p>
<p><a target="_blank" rel="noopener" href="https://video-nerf.github.io/">时空神经辐照度场</a>仅使用时间作为附加输入。需要仔细选择损失才能成功训练此方法以渲染自由视点视频（来自 RGBD 数据）。</p>
<p><a target="_blank" rel="noopener" href="https://www.cs.cornell.edu/~zl548/NSFF/">神经场景流场</a>从 RGB 进行训练，但使用单目深度预测作为先验，并通过输出场景流进行正则化，用于损失。</p>
<p><a target="_blank" rel="noopener" href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a>与 Nerfies 论文非常相似，甚至使用相同的首字母缩略词，但似乎将变形限制为翻译。</p>
<p><a target="_blank" rel="noopener" href="https://yilundu.github.io/nerflow/">NeRFlow</a>是 Arxiv 上的动态 NeRF 变体，它使用了 Nerfies 风格的变形 MLP，整合了跨时间的场景流以获得最终的变形。</p>
<h2 id="Portrait"><a href="#Portrait" class="headerlink" title="Portrait"></a>Portrait</h2><p>除了 Nerfies，还有另外两篇论文关注人物的肖像：</p>
<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2022/08/20/NeRF-2020/8.png" srcset="/img/loading.gif" lazyload width="90%" alt>    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">      Dynamic Neural Radiance Fields 在任务方面与 Nerfies 非常相似，但使用可变形的人脸模型来简化训练和渲染。      </div> </center>

<p><a target="_blank" rel="noopener" href="https://gafniguy.github.io/4D-Facial-Avatars/">DNRF</a>专注于 4D 化身，因此通过在管道中包含可变形的面部模型来施加强烈的归纳偏差。这给出了对动态 NeRF 的<em>参数</em>控制。</p>
<p><a target="_blank" rel="noopener" href="https://portrait-nerf.github.io/">Portrait NeRF</a>创建静态 NeRF 风格的头像，但通过单个 RGB 头像来实现。为了完成这项工作，需要轻量级训练数据。</p>
<h2 id="Relighting"><a href="#Relighting" class="headerlink" title="Relighting"></a>Relighting</h2><p>NeRF 风格方法得到增强的另一个方面是如何处理光照，通常是通过可用于重新光照场景的潜在代码。</p>
<p><a target="_blank" rel="noopener" href="https://nerf-w.github.io/">NeRF-W</a>是 NeRF 的首批后续工作之一，它优化了潜在的外观代码，以便从受控较少的多视图集合中学习神经场景表示。</p>
<p><a target="_blank" rel="noopener" href="https://cseweb.ucsd.edu/~bisai/">神经反射场</a>通过在密度之外添加局部反射模型来改进 NeRF。尽管来自单点光源，但它产生了令人印象深刻的重新照明效果。</p>
<p><a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~pratul/nerv/">NeRV</a>使用第二个“可见性”MLP 来支持任意环境照明和“单次反射”间接照明。</p>
<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2022/08/20/NeRF-2020/NeRD.jpg" srcset="/img/loading.gif" lazyload width="70%" alt>    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">      NeRD 与本节中的其他方法一样，在学习更复杂的局部反射模型以及密度方面超越了 NeRF。NeRD 本身也为训练集中的每个场景学习了一个全局光照模型，如面板 d 所示。      </div> </center>

<p><a target="_blank" rel="noopener" href="https://markboss.me/publication/2021-nerd/">NeRD</a>（神经反射分解）是使用局部反射模型的另一项尝试，此外还为给定场景推断了低分辨率球面谐波照明。</p>
<h2 id="Shape"><a href="#Shape" class="headerlink" title="Shape"></a>Shape</h2><p>潜在代码也可用于对形状先验进行编码。</p>
<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2022/08/20/NeRF-2020/9.png" srcset="/img/loading.gif" lazyload width="80%" alt>    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">      GRAF（和 NeRF 的其他条件变体）为形状和/或外观添加了潜在代码，因此神经体积渲染可以用于生成方式以及推理      </div> </center>

<p><a target="_blank" rel="noopener" href="https://autonomousvision.github.io/graf/">GRAF</a>（辐射场的生成模型）是 NeRF 的条件变体，添加了外观和形状潜在代码，而视点不变性是通过 GAN 风格的训练获得的。</p>
<p><a target="_blank" rel="noopener" href="https://marcoamonteiro.github.io/pi-GAN-website/">pi-GAN</a>类似于 GRAF，但使用了 SIREN 风格的 NeRF 实现，其中每一层都由不同 MLP 的输出调制，该 MLP 接收潜在代码。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/sxyu/pixel-nerf">pixelNeRF</a>更接近于基于图像的渲染，在测试时使用 N 个图像。它基于 PIFu，创建像素对齐的特征，然后在评估 NeRF 风格的渲染器时对其进行插值。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/alextrevithick/GRF">GRF</a>在设置上非常接近 pixelNeRF，但在规范空间而不是视图空间中运行。</p>
<h2 id="Composition"><a href="#Composition" class="headerlink" title="Composition"></a>Composition</h2><p>将上述模型扩展到由许多对象组成的大型场景，一个令人兴奋的新领域是如何将对象组合成体积渲染的场景。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.10379">Neural Scene Graphs</a>在场景图中支持多个以对象为中心的 NeRF 模型。</p>
<p><a target="_blank" rel="noopener" href="https://wentaoyuan.github.io/star/">STaR</a>类似于 Neural Scene Graphs 论文，仅限于单个对象，但在训练时不需要姿势监督。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.12100">GIRAFFE</a>通过让以对象为中心的 NeRF 模型输出特征向量而不是颜色来支持组合，然后通过平均来组合，并以低分辨率渲染到 2D 特征图，然后在 2D 中上采样。</p>
<center class="half">    <img src="/2022/08/20/NeRF-2020/thjHOME\暑假科研训练基础\note_img\nerf\OSF.gif" srcset="/img/loading.gif" lazyload width="300">    <img src="/2022/08/20/NeRF-2020/OSF2.gif" srcset="/img/loading.gif" lazyload width="300"> <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">      对象散射函数 (OSF)，支持组合具有任意对象位置（左）和光照（右）的场景。      </div> </center>

<p><a target="_blank" rel="noopener" href="https://shellguo.com/osf/">以对象为中心的神经场景渲染</a>在以对象为中心的坐标系中学习“对象散射函数”，允许使用蒙特卡洛渲染来组合场景并真实地照亮它们。</p>
<h2 id="Pose-Estimation"><a href="#Pose-Estimation" class="headerlink" title="Pose Estimation"></a>Pose Estimation</h2><p>最后，这篇论文在物体姿态估计的上下文中使用了 NeRF 渲染。</p>
<p><a target="_blank" rel="noopener" href="https://yenchenlin.me/inerf/">iNeRF</a>在姿态估计框架中使用 NeRF MLP，甚至能够通过微调姿态来改进标准数据集上的视图合成。但是，它还不能处理照明。</p>
<center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2022/08/20/NeRF-2020/iNeRF.gif" srcset="/img/loading.gif" lazyload width="70%" alt>    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">      iNERF使用 NeRF 作为姿势优化器中的合成模型      </div> </center>


                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/CVNote/" class="category-chain-item">CVNote</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/NeRF/">#NeRF</a>
      
        <a href="/tags/survey/">#survey</a>
      
        <a href="/tags/CV/">#CV</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>NeRF survey(2020)</div>
      <div>https://thj2333.github.io/2022/08/20/NeRF-2020/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>thj2333</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年8月20日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/06/27/%E8%AE%A1%E7%BB%84%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="计组复习笔记">
                        <span class="hidden-mobile">计组复习笔记</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
